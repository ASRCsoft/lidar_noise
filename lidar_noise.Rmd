---
journal: jhm
layout: twocol
title: Wind lidar signal reception as a hidden Markov process
author1: Andrew N. Other
author2: Fred T. Secondauthor
author3: Jeff Friedman
currentaddress: "Current address: Some other place, Germany"
affiliation: "American Meteorological Society,Boston, Massachusetts"
exauthors: 
  - name: William May
    exaffiliation: Atmospheric Sciences Research Center
    correspondingauthor: "American Meteorological Society, 45 Beacon St., Boston, MA 02108."
    email: \email{groupleader@unknown.uu}
  - name: Kara Sulia
    exaffiliation: Atmospheric Sciences Research Center
    currentaddress: "Current address: Some other place, Canada"
  - name: Jeff Friedman
    exaffiliation: Atmospheric Sciences Research Center
    currentaddress: "Current address: Some other place, Canada"
abstract: |
  Wind lidars are widely deployed to measure atmospheric wind speeds. However, wind speed measurements are corrupted by impulse noise when the lidar's laser backscatter is weaker than environmental background noise. We model lidar signal reception as a hidden Markov process that switches between a noise state and a signal state and draw on image processing research to obtain approximate solutions to the model. Using this model we are able to dramatically improve on the current data filtering methods in use for wind lidars.
bibliography: lidar_noise.bib
output:
  bookdown::pdf_book:
    base_format: rticles::ams_article
header-includes:
  - \hypersetup{draft}
  - \usepackage{subfig}
  - \usepackage{booktabs}
  - \DeclareMathOperator{\median}{median}
  - \DeclareMathOperator{\gauss}{gauss}
---

<!-- `\hypersetup{draft}` had to be included trying to avoid error possibly caused by references split across pages, as discussed here: https://www.overleaf.com/help/246-what-does-slash-pdfendlink-ended-up-in-different-nesting-level-than-slash-pdfstartlink-mean -->

```{r setup, echo=TRUE, include=FALSE}
library(reticulate)
if (file.exists('python_path.txt')) {
  python_path = readLines('python_path.txt')[1]
} else {
  python_path = Sys.which('python3')
}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, cache = FALSE,
                      engine.path = list(python = python_path))
knitr::knit_engines$set(python = reticulate::eng_python)
```

# Introduction

Wind lidars are routinely used in the fields of wind energy and atmospheric science to provide vertical profiles of atmospheric wind speeds [@weitkamp_lidar:_2005; @lang_lidar_2011].

As a signal measuring instrument in which measurements are retrieved via backscatter, background noise accompanies data retrievals. As such, lidar instrumentation include noise-filtering software, typically designed by the lidar manufacturer. Such software include X (cite), Y (cite), and Z (cite) methodologies.

The Leosphere lidar currently has a signal strength threshold built into the data-retrieval software. This methodology filters….<add here the current filtering thresholds>. It is found that while this methodology serves to provide an acceptable range of retrieved data, the limitations of the aforementioned thresholds leave gaps in more precise retrievals of X, Y, and Z, and also leave behind (relatively obvious) noisy data among what is deemed real data.

To more accurately filter wind measurements, we propose a hidden markov random field model of lidar wind measurements. In the model, the measurements switch between noise and wind states. The recorded CNR and radial wind speed values are generated by a function that depends on the state.

In the wind state, the recorded radial wind speeds and CNR values exhibit spatial and temporal autoregression. To account for this our model requires an estimate of the local radial wind speed and CNR values in the wind state. We turn to impulse noise filtering methods developed in image processing research to obtain these estimates.

We apply this method to data collected by Leosphere lidars used by the University at Albany Atmospheric Sciences Research Center as well as the New York State Mesonet. We then validate the resulting wind estimates against radiosonde wind estimates. The reults show that the resulting measurement accuracy is comparable to that of the Leosphere wind estimates below the planetary boundary layer, and greater than the current Leosphere estimates above the planetary boundary layer.

# Background

## Wind lidars


## Current filtering methods

Current classification strategies for lidar data typically involve a combination of signal strength thresholds and spike filters.

Optimal signal strength thresholds can be calculated using detection theory. With information about the distribution of signal strengths in both signal and noise measurements, detection theory describes how to use the measurement's signal strength to derive the likelihood of a measurement being signal or noise. The expected number of false positives and false negatives can also be calculated for a given threshold choice. The chosen threshold is then determined by the relative cost of false positives and false negatives.

Lidar wind estimates behave erratically in the presence of errors in the underlying radial wind speed estimates. This creates a substantial cost for noise incorrectly labeled as radial wind speeds. As a result wind lidar manufacturers set conservative signal strength thresholds that favor false negatives over false positives. The Leosphere WindCube 100S wind lidar selects a threshold to achieve a false alarm rate of 0.25%. This choice minimizes incorrect wind estimates at the cost of a substantial amount of false negatives.

something about spike filters...


## Hidden Markov Random Fields

Markov random fields are commonly used to model image segmentation problems [@li_markov_2009]. The models represent pixels as nodes in a regular network structure. Figure \@ref(fig:networks) shows two common network structures. Each node contains a hidden label which corresponds to an object in an image. Label transition probabilities from a node to a neighboring node are constant throughout the network, thus satisfying Markov properties.

```{r networks, message=FALSE, fig.cap='Standard Markov random field structures. a) Order 1. b) Order 2.', fig.subcap=c('', ''), fig.width=1.5, fig.height=1.5, fig.ncol=2}
library(igraph)

## the grid of nodes
nodes = matrix(1:100, nrow=10, ncol=10)

make_edges = function(nodes, x_offset, y_offset) {
  if (x_offset >= 0) {
    start_nodes = nodes[1:(nrow(nodes) - y_offset),
                        1:(nrow(nodes) - x_offset)]
    end_nodes = nodes[(y_offset + 1):nrow(nodes),
                      (x_offset + 1):nrow(nodes)]
  } else {
    start_nodes = nodes[1:(nrow(nodes) - y_offset),
                        (1 - x_offset):nrow(nodes)]
    end_nodes = nodes[(y_offset + 1):nrow(nodes),
                      1:(nrow(nodes) + x_offset)]
  }
  cbind(as.vector(start_nodes), as.vector(end_nodes))
}


edges_o1 = rbind(make_edges(nodes, 1, 0),
                 make_edges(nodes, 0, 1))
edges_o2 = rbind(make_edges(nodes, 1, 0),
                 make_edges(nodes, 0, 1),
                 make_edges(nodes, 1, 1),
                 make_edges(nodes, -1, 1))
go1 = graph_from_edgelist(edges_o1, directed=F)
go2 = graph_from_edgelist(edges_o2, directed=F)

grid_layout = layout_on_grid(go1, width = 10, height = 10)

par(mar=rep(0, 4))
plot(go1, layout=grid_layout,
     rescale=FALSE, xlim=c(3.5, 7.5), ylim=c(3.5, 7.5),
     vertex.size=30, vertex.color='gray', vertex.label=NA)

par(mar=rep(0, 4))
plot(go2, layout=grid_layout,
     rescale=FALSE, xlim=c(3.5, 7.5), ylim=c(3.5, 7.5),
     vertex.size=30, vertex.color='gray', vertex.label=NA)
```

Observed pixel values are drawn from label-dependent random distributions. The probability of a label for a given pixel can be calculated as a function of the pixel's values and the label probabilities of neighboring nodes.

<!-- For example, neuroimaging researchers use markov random fields to identify brain regions in MRI data, modeling each region as a distinct data generating process with values drawn from a Gaussian distribution. The markov random field incorporates spatial relationships between the voxels to produce more coherent classifications. -->

Existing methods from image segmentation research can estimate hidden markov random field models without autocorrelated error terms. However markov random field models with autocorrelated error terms are intractable. Therefore to estimate the model we have to remove the autocorrelated error terms.


## Image Denoising

A wide variety of filters for predicting local pixel values are available in image processing research which we could potentially use to estimate local space/time error terms. We choose the median filter due to its robustness to “salt and pepper” noise. Salt and pepper noise in degraded images takes on extreme high (white salt) and low (black pepper) values. Typically the median filter is used to replace salt and pepper noise pixels with a more reasonable value derived from neighboring pixels. Given the extreme variance of radial wind speed noise measurements, methods designed for salt and pepper noise hold promise for dealing with lidar noise.


## New York State Mesonet

The New York State Mesonet is a network of 126 weather stations across the state of New York, created as part of the New York State Early Warning Weather Detection System established by the Department of Homeland Security [@nys_mesonet_nys_nodate]. 17 sites contain additional profiling instruments, including the Leosphere WindCube 100S wind lidar. The lidars are primarily run in DBS (Doppler Beam Swinging) mode, which records radial wind speeds from a vertical profile and 4 profiles 15 degrees off from vertical in each of the 4 cardinal directions. Radial wind speeds from different directions can be combined to estimate the u, v, and w components of the wind speed.

Before deployment at the 17 profiler sites, the lidars were tested on the roof of the CESTM building at the SUNY Polytechnic Institute. The CESTM building also houses the Atmospheric Sciences Research Center and the Albany, NY office of the National Weather Service, which releases radiosondes twice a day from the CESTM roof.

The first lidar began testing in April 2015. All lidars were deployed to their respective Mesonet sites as of April 2018.

# Model

```{python}
# add the current folder to the import path
import sys
sys.path.append('.')
# import modules
import warnings
import numpy as np
warnings.simplefilter("ignore")
import xarray as xr
import rasppy
from rasppy.hmrf import make_segmentation, LidarSamples
from scipy.ndimage import median_filter, gaussian_filter

def make_segmentations(lidar, **seg_args):
    # make segmentation objects for a lidar in DBS mode
    lidar2 = lidar.rasp.los_format()
    samples = []
    for i in range(lidar2.dims['LOS']):
        rws = lidar2['RWS'].sel(LOS=i).swap_dims({'scan': 'Time'})
        cnr = lidar2['CNR'].sel(LOS=i).swap_dims({'scan': 'Time'})
        samples.append(make_segmentation(rws, cnr, **seg_args))
        
    return samples

def estimate_status(lidar, freeze=(), **seg_args):
    '''Estimate measurement status using the HMRF model.'''
    samples = make_segmentations(lidar, beta=.4)
    ls = LidarSamples(samples)
    ls.run(10, freeze=freeze)
    
    lidar2 = lidar.rasp.los_format()
    lidar2['Status'] = (lidar2['RWS'].dims, np.empty(lidar2['RWS'].shape))
    for i in range(lidar2.dims['LOS']):
        lidar0 = lidar2.sel(LOS=i).swap_dims({'scan': 'Time'})
        da = xr.concat([lidar0['RWS'], lidar0['CNR']], 'series').transpose('Time', 'Range', 'series')
        scan_is_complete = ~np.isnan(da).any(['Range', 'series'])
        scan_is_complete = scan_is_complete.swap_dims({'Time': 'scan'})
        lidar2['Status'][:,scan_is_complete,i] = ls.samples[i].ppm[:,:,0].transpose()
        lidar2['Status'][:,~scan_is_complete,i] = np.NaN
        
    lidar2['Status'].attrs['mu'] = ls.mu
    lidar2['Status'].attrs['sigma'] = ls.sigma

    return lidar2['Status'].stack(profile=['scan', 'LOS']).swap_dims({'profile': 'Time'}).drop('profile')

# get lidar data for LOS==0
lidar = xr.open_dataset('cestm_roof80_20171003.nc').sel(Time=slice('2017-10-03 03:00', '2017-10-03 09:00'))
lidar['hmrf'] = estimate_status(lidar)
lidar0 = lidar.rasp.los_format().sel(LOS=0)
lidar0.swap_dims({'scan': 'Time'})

# apply filters
lidar0['filtered_RWS'] = (lidar0['RWS'].dims, median_filter(lidar0['RWS'], size=(7, 59)))
lidar0['filtered_CNR'] = (lidar0['CNR'].dims, gaussian_filter(lidar0['CNR'], sigma=(1, 2)))

# organize for R
rws = lidar0['RWS'].values
frws = lidar0['filtered_RWS'].values
cnr = lidar0['CNR'].values
fcnr = lidar0['filtered_CNR'].values
times = lidar0.coords['Time'].values
ranges = lidar0.coords['Range'].values

rws_spike = lidar0['RWS'].where(np.abs(lidar0['RWS'] - lidar0['filtered_RWS']) < 1).values
rws_cnr = lidar0['RWS'].where(lidar0['Status']).values
rws_hmrf = lidar0['RWS'].where(lidar0['hmrf'] > .5).values
```

For each line of sight, we treat the height/time array of measurements as the output of a 2-dimensional hidden Markov random field connected in a regular lattice network of order 2. Measurements switch between noise and wind states. Radial wind speed noise is drawn from a uniform distribution over the range of possible radial wind speed values. For the WindCube 100S, this range is approximately (-30, 30) m s^-1^. CNR noise is drawn from a normal distribution with a constant mean and standard deviation estimated from the data.

Both RWS and CNR measurements in the wind state are generated by a true atmospheric value and normally distributed measurement error:
$$\begin{aligned}
&RWS_{lidar, i} = RWS_{atmosphere, i} + \epsilon_{RWS, i}\\
&CNR_{lidar, i} = CNR_{atmosphere, i} + \epsilon_{CNR, i}\\
&\epsilon_{RWS, i} \sim N(0, \sigma^2_{RWS})\\
&\epsilon_{CNR, i} \sim N(0, \sigma^2_{CNR})
\end{aligned}$$
where $i$ is an index of the measurement array.

Since the true atmospheric values are unknown, we cannot use these equations to calculate state probabilities. Instead we rely on the spatial and temporal autoregression among neighboring measurements to estimate the true atmospheric values. Because weather conditions typically change more slowly than the frequency of lidar measurements, both radial wind speed and CNR exhibit strong autoregressive properties. However this autoregressive property only applies to measurements that are also in the wind state.

The problem of estimating a local value in the presence of noise is isomorphic to the task of image denoising. We therefore borrow successful methods from image denoising to generate robust estimates of the true atmospheric values:
$$\begin{aligned}
RWS_{atmosphere, i} &= median_i + \epsilon_{median, i}\\
CNR_{atmosphere, i} &= gauss_i + \epsilon_{gauss, i}
\end{aligned}$$
where $median_i$ is the result of a median filter applied to radial wind speeds in a window around measurement $i$, $gauss_i$ is the result of a gaussian filter applied to CNR values in a window around measurement $i$, and $\epsilon_{median, i}$ and $\epsilon_{gauss, i}$ are the respective estimation errors resulting from these methods. Figure \@ref(fig:filters) shows the RWS and CNR values before and after filtering.

```{r filters, message=FALSE, fig.cap='Raw and filtered CNR and RWS values for October 3rd, 2017. a) Raw and filtered RWS values. b) Raw and filtered CNR values.', fig.subcap=c('', ''), fig.env="figure*", fig.height=2, fig.ncol=1}
library(reshape2)
library(ggplot2)
library(scales)
library(viridis)

melt_from_py = function(a) {
  df = melt(a)
  df$Range = py$ranges[df$Var1]
  df$Time = py$times[df$Var2]
  df
}

ggheatmap = function(df, name, limits, diverging) {
  p = ggplot(df, aes(x=Time, y=Range, fill=value, color=value)) +
    geom_tile() +
    scale_x_datetime('Hour [UTC]', date_labels='%H', expand = c(0, 0)) +
    scale_y_continuous('Range [m]', expand = c(0, 0))
  if (diverging) {
    p + scale_fill_gradient2(name, low=muted("blue"), high=muted("red"),
                             limits = limits, oob=squish) +
      scale_color_gradient2(name, low=muted("blue"), high=muted("red"),
                            limits = limits, oob=squish)
  } else {
    p + scale_fill_viridis(name, limits=limits, oob=squish) +
      scale_color_viridis(name, limits=limits, oob=squish)
  }
}

# make the plots
rws = melt_from_py(py$rws)
rws$Filter = 'Raw'
frws = melt_from_py(py$frws)
frws$Filter = 'Median'
combined_rws = rbind(rws, frws)
combined_rws$Filter = factor(combined_rws$Filter, levels=c('Raw', 'Median'))
ggheatmap(combined_rws, 'RWS [m/s]', c(-5, 5), T) +
  facet_wrap(~Filter) +
  theme(panel.spacing = unit(1, "lines"))

cnr = melt_from_py(py$cnr)
cnr$Filter = 'Raw'
fcnr = melt_from_py(py$fcnr)
fcnr$Filter = 'Gaussian'
combined_cnr = rbind(cnr, fcnr)
combined_cnr$Filter = factor(combined_cnr$Filter, levels=c('Raw', 'Gaussian'))
ggheatmap(combined_cnr, 'CNR [dB]', c(-35, -15), F) +
  facet_wrap(~Filter) +
  theme(panel.spacing = unit(1, "lines"))
```

We can then express the distribution of measurements in the wind state as a function of known values:
$$\begin{aligned}
&RWS_{lidar, i} = median_i + \epsilon_{RWS + median, i}\\
&CNR_{lidar, i} = gauss_i + \epsilon_{CNR + gauss, i}
\end{aligned}$$

Error distributions for the combined errors are more erratic and harder to specify. For simplicity we assume they are drawn from a normal distribution. Appendix 1 shows that the model misspecification resulting from this choice has little effect on measurement classifications. Table \@ref(tab:distributions) shows the distributions assigned to CNR and RWS values in each state.

```{r distributions}
text_tbl = data.frame(Wind=c('$N(\\median_i, \\sigma^2_{RWS + median})$',
                             '$N(\\gauss_i, \\sigma^2_{CNR + gauss})$'),
                      Noise=c('$U(\\min(RWS), \\max(RWS))$',
                              '$N(\\mu_{noise}, \\sigma^2_{noise})$'),
                      row.names=c('RWS', 'CNR'))
knitr::kable(text_tbl, booktabs=T, escape = F,
  caption = 'RWS and CNR distributions in different states.')
```

The model can be solved with existing hidden Markov random field algorithms. We use the variational expectation maximization approach introduced by @roche_convergence_2011 and implemented in the NIPY Python package.


# Results

30% more data, less noise

```{r filtercomparison, message=FALSE, fig.cap='Results from RWS filters.', fig.env="figure*", fig.height=2, out.extra = ""}
# have to include `out.extra = ""` in this chunk to convince rmarkdown to write
# directly to latex
spike_df = melt_from_py(py$rws_spike)
spike_df$Filter = 'Spike'
cnr_df = melt_from_py(py$rws_cnr)
cnr_df$Filter = 'CNR Threshold'
hmrf_df = melt_from_py(py$rws_hmrf)
hmrf_df$Filter = 'HMRF'
filter_df = rbind(spike_df, cnr_df, hmrf_df)
ggheatmap(filter_df, 'RWS [m/s]', c(-5, 5), T) +
  facet_wrap(~ Filter) +
  theme(panel.spacing = unit(1, "lines"))
```


# Validation

We follow the method outlined in @kumer_comparison_2014 to compare wind lidar estimates to radiosonde wind speed estimates. They compare correlations of wind speed and wind direction from two types of lidars with measurements from a radiosonde launched from a site 2 kilometers away. Because correlations depend on the height of the measurement, they calculate correlations independently for different heights.

We plot Pearson correlation coefficients in figure \@ref(fig:corr). Our results for the CNR threshold method closely match @kumer_comparison_2014 results for the WindCube 100S, which also uses a CNR threshold to filter data. In particular, we find that at lower altitudes the correlation is weaker, at about $R=.9$, possibly due to swinging motions of the radiosonde sensor. We also observe a drop in the correlation around 1300m, around the top of the cloud level, where valid measurements are often interspersed with noise. @kumer_comparison_2014 do not report correlations above 2500m. At these high ranges we record another drop in correlation, possibly due to the radiosonde drifting away from the CESTM building.

The hidden markov random field wind correlations are similar, with the exception that the sharp dips in correlations (that we hypothesize are due to valid measurements interspersed with noise) do not appear. The results suggest that data filtered with the hidden markov random field is comparable in quality to data filtered with the CNR threshold method when only small amounts of noise are present.

```{r corr, fig.cap='Pearson correlation of lidar horizontal wind speeds and radiosonde wind speeds at different heights.', fig.width=3.17, fig.height=2.5}
library(reshape2)
library(ggplot2)

cnr_df = read.csv('validation/cnr_stats.csv')
hmrf_df = read.csv('validation/hmrf_stats.csv')
ranges = seq(100, 3000, by=25)

corr_df = data.frame(Range=ranges, CNR=cnr_df$corr_u, HMRF=hmrf_df$corr_u)
lcorr = melt(corr_df, id.vars='Range', variable.name='Method',
             value.name='Correlation Coefficient')
names(lcorr)[1] = 'Range [m]'
ggplot(lcorr, aes(x=`Range [m]`, y=`Correlation Coefficient`, color=Method)) +
  geom_line() + scale_color_discrete('Method:') + theme(legend.position="bottom")
```


# Case Study: Lake Winds

something about lakes and winds


# Conclusion

lidar researchers should borrow more from image research


# References {-}

<!-- this tells pandoc to add references here instead of at the end of the document -->
<div id="refs"></div>


# Appendix 1 {-}

use order statistics to show reliability of median filter, citing @glueck_fast_2008


<!-- # AMS formatting notes -->

<!-- ## Secondary headings {-} -->

<!-- Secondary headings labeled with letters are formatted using the `## Secondary headings {-}` for a single subsection within a section or `## Secondary headings` for multiple subsections within one section. -->

<!-- ### Tertiary headings {-} -->

<!-- Tertiary headings are formatted using the `### Tertiary headings {-}` for single a subsubsection within a subsection or `### Tertiary headings` for multiple subsubsections within a subsection.  -->

<!-- \paragraph*{Quaternary headings} -->
<!-- Quaternary headings are formatted using the `\paragraph*{Quaternary headings}` for a single paragraph within a subsubsection or `\paragraph{Quaternary headings}` for multiple paragraphs within a subsection. -->

<!-- # Citations -->

<!-- Citations to standard references in text should consist of the name of the author and the year of publication, for example, @poe12 or [@poe12;@alexander:2002;@Gershunov2012] using the appropriate `@key` or `[@key]` commands, respectively. A variety of citation formats can be used with the natbib package; however, the AMS prefers that authors use only the `@key` and `[@key]` commands. References should be entered in the references.bib file. For a thorough discussion of how to enter references into the references.bib database file following AMS style, please refer to the **AMS_Refs.pdf** document included in this package. -->

<!-- # Formatting math -->

<!-- The following sections will outline the basic formatting rules for mathematical symbols and units.  In addition, a review of the amspaper.tex file will show how this is done with the use of \LaTeX\ commands.  The AMS template provides the American Mathematical Society math, font, symbol, and boldface packages for use in math mode. -->

<!-- ## Mathematical symbols -->

<!-- Symbols must be of the same font style both in text discussion and in displayed equations or terms (and figures should be prepared to match). Scalar single-character symbols are set italic, Greek, or script. Examples are $u$, $L$ [note that $\upsilon$ (Greek upsilon) is used instead of *v* (italic "vee") to avoid confusion with $\nu$ (Greek nu) often used for viscosity; this is handled automatically when in \LaTeX\ math mode], $w$, $x$, $y$, $z$, $f$, $g$, $r$, indices such as $i$ or $j$, and constants such as $C_D$, $k$, or $K$. Multiple-character scalar variables, abbreviations, nondimensional numbers, and acronyms for variables are set regular nonitalic: $\mathrm{LWC}$, $\mathrm{Re}$, $\mathrm{Ro}$, $\mathrm{BT}$, $\mathrm{abs}$, $\mathrm{obs}$, $\mathrm{max}$, $\mathrm{min}$, $\mathrm{Re}$/$\mathrm{Im}$ (real/imaginary), etc. For vectors, use boldface nonitalic Times Roman as in $\mathbf{V}$, $\mathbf{v}$, or $\mathbf{x}$, and $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ unit vectors. Do not use the \LaTeX\ $\backslash vec$ command to denote vectors. For matrix notation, use nonitalic boldface Arial (or sans serif) font as in $\pmb{\mathsf{A}}$, $\pmb{\mathsf{B}}$, or $\pmb{\mathsf{M}}$. Note that you will need to use the $\backslash$pmb command for boldface sans serif; the $\backslash$bm command will not work. All mathematical operator abbreviations/acronyms are set lowercase regular Roman font, except $O$ (on the order of): $\sin$, $\cos$, $\tan$, $\tanh$, $\mathrm{cov}$, $\Pr$ (for probability; note same as Prandtl number), $\mathrm{const}$ (for constant), $\mathrm{c.c.}$ (complex conjugate). -->

<!-- ## Units -->

<!-- Units are always set on a single line with a space separating the denominator, which is set with a superscript $-1$, $-2$, and so on, rather than using a slash for "per." Examples are g kg$^{-1}$, m$^2$ s$^{-1}$, Wm$^{-2}$, g m$^{-3}$, and m s$^{-1}$ (note that ms$^{-1}$ is the unit for "per millisecond"). -->

<!-- ## Equations -->

<!-- Brief equations or terms set inline in text must be set as a single-line expression because page proofs are not double spaced, for example, $\rho^{-1}p/x$ or $(1/{\rho})p/x$ or $(a-b)/(c+d)$; that is, use a superscript $-1$ for the denominator. In case of a more complicated term or equation, it should be set as an unnumbered display equation, such as -->

<!-- $$ -->
<!-- x=\frac{2b\pm\sqrt{b^{2}-4ac}}{2c}. -->
<!-- $$ -->

<!-- Otherwise, numbered display equations can be entered using the appropriate equation command, such as  -->

<!-- \begin{equation} -->
<!-- x=\frac{2b\pm\sqrt{b^{2}-4ac}}{2c}.   -->
<!-- \end{equation} -->

<!-- Lists of equations are punctuated as written English, and commas, semicolons, and periods are placed where appropriate. Conjunctions such as "and", "while", "when", or "for" are also typically placed before the final element in a mathematical phrase, as befits the intended mathematical meaning.   -->

<!-- ## Figures and tables -->

<!-- The AMS prefers that all figures and tables are placed **at the end of the document** prior to submission. A list of tables and a list of figures will appear near the end of the PDFfile, before the actual tables and figures. These lists are necessary for submission. -->

<!-- For appendix figures and tables, special commands are needed to manually change the numbering to ensure that each appendix figure or table is numbered as part of the respective appendix and not as a continuation of the main paper. Use the command `\appendcaption{}` instead of the usual `caption{}` to adjust the  numbering; for example, for Table A1, you would use the command `\appendcaption{A1}`. -->

<!-- Note that the normal `\ref{}` command cannot be used to cite appendix figures and tables as the numbering will be incorrect. Callouts for appendix figures and tables in the text will need to be written out as plain text, for example, Fig. A1 and Table A1. -->

<!-- ### Figures -->

<!-- The insertion of a sample figure (Fig. \ref{f1})   -->
<!-- and caption is given below (in the .tex document) and at the end of the document. Standard figure sizes are 19 (one column), 27, 33, and 39 (two columns) picas. -->

<!-- \begin{figure}[h] -->
<!--  \centerline{\includegraphics[width=19pc]{figure01.pdf}} -->
<!--   \caption{Enter the caption for your figure here.  Repeat as -->
<!--   necessary for each of your figures.}\label{f1} -->
<!-- \end{figure} -->


<!-- ### Tables -->
<!-- Each table must be numbered, provided with a caption, and mentioned specifically in the text.  -->
<!-- See below (in the .tex document) and at the end of the document for the formatting of a sample table (Table -->
<!-- \ref{t1}). -->

<!-- \begin{table}[h] -->
<!-- \caption{This is a sample table caption and table layout.}\label{t1} -->
<!-- \begin{center} -->
<!-- \begin{tabular}{ccccrrcrc} -->
<!-- \topline -->
<!-- $N$ & $X$ & $Y$ & $Z$\\ -->
<!-- \midline -->
<!--  0000 & 0000 & 0010 & 0000 \\ -->
<!--  0005 & 0004 & 0012 & 0000 \\ -->
<!--  0010 & 0009 & 0020 & 0000 \\ -->
<!--  0015 & 0016 & 0036 & 0002 \\ -->
<!--  0020 & 0030 & 0066 & 0007 \\ -->
<!--  0025 & 0054 & 0115 & 0024 \\ -->
<!-- \botline -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- \end{table} -->

<!-- \acknowledgments -->
<!-- Keep acknowledgments (note correct spelling: no e between the g and m) as brief as possible. In general, acknowledge only direct help in writing or research. Financial support (e.g., grant numbers) for the work done, for an author, or for the laboratory where the work was performed is best acknowledged here rather than as footnotes to the title or to an author's name. Contribution numbers (if the work has been published by the author's institution or organization) should be included as footnotes on the title page, -->
<!-- not in the acknowledgments. -->

<!-- Please use The authors thank \ldots rather than The authors would like to thank \ldots. -->

<!-- The author thanks Mats Dahlgren for version one of \textsf{achemso}, and Donald Arseneau for the code taken from \textsf{cite} to move citations after punctuation. Many users have provided feedback on the class, which is reflected in all of the different demonstrations shown in this document. -->

<!-- \appendix[A]  -->

<!-- \appendixtitle{Title of Appendix} -->

<!-- ## Appendix section -->

<!-- The AMS template allows authors to format an unlimited number of appendixes. To format a single appendix, use the `\appendix` command with no additional argument. Otherwise, add the appropriate one-letter argument to the `\appendix` command (e.g. `\appendix[A]`, `\appendix[B]`, `\appendix[C]`, etc.) corresponding to the appropriate appendix.  -->

<!-- The title of the appendix can be formatted using the `\appendixtitle{} ` command. The `##` , `###` and `\paragraph` commands are used to create sections within the appendix. (Note that the appendix title takes the place of `#` in the appendix, so the first section should begin with `##` instead of `#`.) -->

<!-- Equations are automatically numbered appropriately for each appendix. Here is an example of the first equation in appendix A, automatically labeled (\ref{eq:1}):  -->

<!-- \begin{equation} -->
<!-- \label{eq:1} -->
<!-- x=\frac{2b\pm\sqrt{b^{2}-4ac}}{2c}.   -->
<!-- \end{equation} -->

<!-- For appendix figures and tables, special commands are needed to manually change the numbering to ensure that each appendix figure or table is numbered as part of the appendix and not as a continuation of the main paper. Use the command `\appendcaption{}` instead of the usual `\caption{}` to adjust the numbering; for example, for Table A1, you would use the command `\appendcaption{A1}`. In-text callouts for each appendix figure and table will need to be written as plain text;the usual `\ref{}` command cannot be used. -->

<!-- # References {-} -->
<!-- \bibliography{references} -->

<!-- \begin{table} -->
<!-- \appendcaption{A1}{Here is the appendix table caption.} -->
<!-- \centering -->
<!-- \begin{tabular}{ccc} -->
<!-- \topline -->
<!-- $1$ & $2$ & $3$ \\ -->
<!-- \midline -->
<!-- a&b&c \\ -->
<!-- d&e&f \\ -->
<!-- \botline -->
<!-- \end{tabular} -->
<!-- \end{table} -->

<!-- \begin{figure} -->
<!-- \centerline{(illustration here)} -->
<!-- \appendcaption{A1}{Here is the appendix figure caption.} -->
<!-- \end{figure} -->

<!-- \begin{figure} -->
<!-- \centerline{(illustration here)} -->
<!-- \appendcaption{B1}{Here is the appendix figure caption.} -->
<!-- \end{figure} -->
